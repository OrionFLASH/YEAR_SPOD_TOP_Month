# Рекомендации по оптимизации производительности

## Анализ текущей производительности

По логам видно, что загрузка файлов занимает **~22 минуты** для 36 файлов (12 файлов × 3 группы). Каждый файл OD загружается по **1.5-2 минуты**, что указывает на узкое место в загрузке Excel файлов.

### Текущие проблемы производительности:

1. **Загрузка Excel файлов** - основное узкое место
   - Файлы очень большие (150-200 тысяч строк)
   - Загрузка каждого файла занимает 1.5-2 минуты
   - Применение drop_rules удаляет сотни тысяч строк, но это происходит после загрузки

2. **Нормализация данных** - применяется через `apply()` для каждой строки
   - Хотя уже частично оптимизировано, можно улучшить

3. **Применение фильтров** - происходит после загрузки всех данных

## Реализованные оптимизации

### 1. Оптимизация загрузки Excel файлов
- ✅ Добавлен параметр `usecols` для загрузки только нужных колонок
- ✅ Это позволяет загружать только 6 колонок вместо всех колонок в файле
- **Ожидаемое ускорение:** 2-5x (в зависимости от количества колонок в исходных файлах)

### 2. Векторизация нормализации
- ✅ Частично векторизована нормализация табельных номеров и ИНН
- ✅ Используются векторизованные операции pandas вместо циклов
- **Ожидаемое ускорение:** 1.5-3x

### 3. Оптимизация сообщений логирования
- ✅ Сообщения о загрузке файлов перенесены с DEBUG на INFO
- ✅ Теперь выводятся в консоль для мониторинга прогресса

## Дополнительные рекомендации по оптимизации

### 1. Параллельная загрузка файлов (высокий приоритет)
**Проблема:** Файлы загружаются последовательно, что занимает много времени.

**Решение:** Использовать `concurrent.futures.ThreadPoolExecutor` или `multiprocessing` для параллельной загрузки файлов.

**Ожидаемое ускорение:** 3-6x (в зависимости от количества ядер CPU и скорости диска)

**Пример реализации:**
```python
from concurrent.futures import ThreadPoolExecutor, as_completed

def load_file_wrapper(args):
    file_path, group_name = args
    return self._load_file(file_path, group_name)

# В load_all_files():
with ThreadPoolExecutor(max_workers=4) as executor:
    futures = {}
    for item in items:
        file_path = group_path / item.file_name
        if file_path.exists():
            future = executor.submit(self._load_file, file_path, group)
            futures[future] = (item.file_name, item.label)
    
    for future in as_completed(futures):
        file_name, label = futures[future]
        try:
            df = future.result()
            if df is not None and not df.empty:
                self.processed_files[group][file_name] = df
        except Exception as e:
            self.logger.error(f"Ошибка при загрузке {file_name}: {str(e)}")
```

**Ограничения:**
- Excel файлы могут быть заблокированы при параллельном чтении (зависит от библиотеки)
- Нужно проверить совместимость с openpyxl/xlsxwriter
- Может потребоваться использование процессов вместо потоков

### 2. Chunking для очень больших файлов (средний приоритет)
**Проблема:** Загрузка файлов по 200 тысяч строк в память сразу может быть медленной.

**Решение:** Загружать файлы частями (chunks) и обрабатывать по частям.

**Ожидаемое ускорение:** 1.5-2x для очень больших файлов (>500k строк)

**Пример реализации:**
```python
chunk_size = 50000
chunks = []
for chunk in pd.read_excel(file_path, chunksize=chunk_size, **read_params):
    # Применяем фильтры к каждому chunk
    chunk = self._apply_drop_rules(chunk, config["drop_rules"], file_path.name, group_name)
    chunks.append(chunk)
df = pd.concat(chunks, ignore_index=True)
```

**Ограничения:**
- `pd.read_excel` не поддерживает `chunksize` напрямую
- Нужно использовать альтернативные методы (например, чтение по строкам)

### 3. Кэширование загруженных данных (низкий приоритет)
**Проблема:** При повторных запусках файлы загружаются заново.

**Решение:** Сохранять загруженные и отфильтрованные данные в pickle/csv для быстрой загрузки при повторных запусках.

**Ожидаемое ускорение:** 10-50x для повторных запусков (если данные не изменились)

**Пример реализации:**
```python
cache_file = cache_dir / f"{file_path.stem}.pkl"
if cache_file.exists() and cache_file.stat().st_mtime > file_path.stat().st_mtime:
    df = pd.read_pickle(cache_file)
else:
    df = pd.read_excel(file_path, **read_params)
    # Применяем фильтры
    df = self._apply_drop_rules(df, ...)
    df.to_pickle(cache_file)
```

### 4. Оптимизация применения drop_rules (средний приоритет)
**Проблема:** Хотя drop_rules уже векторизованы, для очень больших файлов можно оптимизировать дальше.

**Решение:** 
- Применять drop_rules до нормализации (уже реализовано)
- Использовать более эффективные методы фильтрации
- Кэшировать результаты проверок

**Ожидаемое ускорение:** 1.2-1.5x

### 5. Использование более быстрых библиотек для чтения Excel (высокий приоритет)
**Проблема:** `pandas.read_excel` использует openpyxl, который может быть медленным для больших файлов.

**Решение:** 
- Попробовать `pyxlsb` для .xlsb файлов (быстрее)
- Использовать `xlrd` для старых .xls файлов
- Рассмотреть конвертацию в CSV для ускорения

**Ожидаемое ускорение:** 2-5x (зависит от формата файлов)

### 6. Оптимизация памяти (средний приоритет)
**Проблема:** Загрузка всех файлов в память одновременно может быть проблематичной.

**Решение:**
- Обрабатывать файлы по одному и сразу агрегировать данные
- Использовать более эффективные типы данных (например, категории для строковых колонок)
- Удалять промежуточные данные после использования

**Ожидаемое ускорение:** 1.2-1.5x (за счет лучшего использования кэша CPU)

## Приоритеты оптимизации

1. **Высокий приоритет:**
   - ✅ Оптимизация загрузки Excel (usecols) - **РЕАЛИЗОВАНО**
   - Параллельная загрузка файлов - **РЕКОМЕНДУЕТСЯ**
   - Использование более быстрых библиотек - **РЕКОМЕНДУЕТСЯ**

2. **Средний приоритет:**
   - Chunking для больших файлов
   - Оптимизация применения drop_rules
   - Оптимизация памяти

3. **Низкий приоритет:**
   - Кэширование загруженных данных

## Ожидаемый результат после всех оптимизаций

- **Текущее время:** ~22 минуты для загрузки 36 файлов
- **После оптимизаций:** ~3-5 минут (ускорение 4-7x)
- **С кэшированием:** ~10-30 секунд для повторных запусков

## Рекомендации по мониторингу

1. Добавить измерение времени для каждого этапа обработки
2. Логировать время загрузки каждого файла
3. Отслеживать использование памяти
4. Сравнивать производительность до и после оптимизаций

## Примечания

- Все оптимизации должны быть протестированы на реальных данных
- Некоторые оптимизации могут требовать дополнительных зависимостей
- Параллельная загрузка может быть ограничена возможностями библиотек Excel
