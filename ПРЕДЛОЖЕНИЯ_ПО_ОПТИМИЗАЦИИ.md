# Предложения по оптимизации производительности

## Анализ производительности по логу

**Общее время выполнения:** 44 минуты 34 секунды

### Временные затраты по этапам:

1. **Загрузка файлов:** ~22 минуты
   - Каждый файл OD: 1-2 минуты (150-200 тыс. строк)
   - Файлы RA/PS: быстрее (12-13 тыс. строк)

2. **Сбор уникальных табельных номеров:** ~22 минуты
   - Очень медленно для больших файлов

3. **Подготовка сводных данных:** ~21 минута 28 секунд
   - Возможно связано с предыдущим этапом

4. **Подготовка расчетных данных:** 3 секунды ✓
5. **Сохранение:** 53 секунды ✓

---

## Критические проблемы производительности

### Проблема 1: Метод `_apply_drop_rules` - использование `apply()`

**Текущая реализация:**
```python
mask_forbidden = cleaned[rule.alias].apply(is_forbidden)
```

**Проблема:**
- `apply()` вызывает функцию Python для каждой строки (150-200 тыс. раз)
- Очень медленно для больших DataFrame
- Для файла с 200 тыс. строк это может занимать 1-2 минуты

**Решение:**
- Использовать векторизацию pandas вместо `apply()`
- Преобразовать значения в строки один раз, затем использовать `isin()` или `str.contains()`
- Ожидаемое ускорение: **10-50x** для больших файлов

**Пример оптимизации:**
```python
# Вместо apply():
mask_forbidden = cleaned[rule.alias].apply(is_forbidden)

# Использовать векторизацию:
col_str = cleaned[rule.alias].astype(str).str.strip().str.lower()
mask_forbidden = col_str.isin(forbidden)
```

---

### Проблема 2: Метод `collect_unique_tab_numbers` - неэффективный выбор ТБ/ГОСБ

**Текущая реализация:**
- Для каждого табельного номера выполняется поиск через `mask` в исходном DataFrame
- Это очень медленно для больших файлов (150-200 тыс. строк)

**Проблема:**
```python
for tab_num in grouped[tab_col].unique():
    tab_data = grouped[grouped[tab_col] == tab_num]
    # ...
    mask = df_normalized[tab_col] == max_row[tab_col]
    # ... еще несколько масок
    matching_indices = df_normalized[mask].index
```

**Решение:**
- Использовать `groupby().idxmax()` для выбора индекса строки с максимальным значением
- Или использовать `groupby().nlargest(1)` для получения первой строки с максимальным значением
- Создать индекс по табельному номеру один раз, затем использовать его для быстрого поиска
- Ожидаемое ускорение: **5-20x**

**Пример оптимизации:**
```python
# Вместо цикла по каждому табельному номеру:
# Использовать groupby с агрегацией и idxmax
grouped = df_normalized.groupby([tab_col, tb_col, gosb_col])[indicator_col].sum()
max_indices = grouped.groupby(level=0).idxmax()  # Получаем индексы с максимальными значениями
```

---

### Проблема 3: Условное удаление в `_apply_drop_rules` - цикл по индексам

**Текущая реализация:**
```python
for idx in cleaned[mask_forbidden].index:
    row = cleaned.loc[idx]
    # ... проверки для каждой строки
```

**Проблема:**
- Цикл по индексам с доступом к строкам через `loc[idx]` очень медленный
- Для файла с 200 тыс. строк и 100 тыс. запрещенных значений это может занимать минуты

**Решение:**
- Использовать векторизацию pandas для условного удаления
- Группировать по ИНН/ТН и проверять наличие других значений через `groupby()`
- Ожидаемое ускорение: **10-100x** в зависимости от количества запрещенных значений

---

### Проблема 4: Множественные копии DataFrame

**Текущая реализация:**
- `df.copy()` вызывается несколько раз в разных местах
- Для больших DataFrame (200 тыс. строк) это может занимать секунды

**Решение:**
- Минимизировать количество копий
- Использовать `inplace=True` где возможно
- Ожидаемое ускорение: **1.5-3x**

---

## Предложения по оптимизации (приоритет)

### Критический приоритет (ожидаемое ускорение: 20-50x)

1. **Оптимизировать `_apply_drop_rules`:**
   - Заменить `apply(is_forbidden)` на векторизацию pandas
   - Использовать `str.lower().isin()` вместо цикла
   - Ожидаемое ускорение загрузки: **10-50x**

2. **Оптимизировать выбор ТБ/ГОСБ в `collect_unique_tab_numbers`:**
   - Использовать `groupby().idxmax()` вместо цикла с mask
   - Создать индекс один раз для быстрого поиска
   - Ожидаемое ускорение: **5-20x**

### Высокий приоритет (ожидаемое ускорение: 10-100x)

3. **Оптимизировать условное удаление в `_apply_drop_rules`:**
   - Заменить цикл по индексам на векторизацию
   - Использовать `groupby()` для проверки наличия других значений
   - Ожидаемое ускорение: **10-100x** (если используется условное удаление)

### Средний приоритет (ожидаемое ускорение: 1.5-3x)

4. **Минимизировать копии DataFrame:**
   - Убрать лишние `df.copy()`
   - Использовать `inplace=True` где возможно

5. **Оптимизировать статистику загрузки:**
   - Уже частично оптимизировано, но можно улучшить
   - Использовать `nunique()` без дополнительных преобразований

---

## Ожидаемый результат после оптимизаций

**Текущее время:** 44 минуты 34 секунды

**После оптимизаций:**
- Загрузка файлов: 22 мин → **1-3 минуты** (ускорение 7-22x)
- Сбор табельных номеров: 22 мин → **1-2 минуты** (ускорение 11-22x)
- Подготовка сводных данных: 21 мин → **<1 минуты** (ускорение >21x)

**Общее ожидаемое время:** **3-6 минут** (ускорение **7-15x**)

---

## Риски и ограничения

1. **Векторизация может изменить поведение:**
   - Нужно тщательно протестировать на реальных данных
   - Особенно условное удаление с проверкой по ИНН/ТН

2. **Совместимость:**
   - Нужно убедиться, что оптимизации не ломают существующую логику

3. **Сложность реализации:**
   - Оптимизация условного удаления может быть сложной
   - Нужно правильно обработать все edge cases

---

## Рекомендации

**Рекомендуется начать с:**
1. Оптимизация `_apply_drop_rules` с векторизацией (критично, просто)
2. Оптимизация выбора ТБ/ГОСБ в `collect_unique_tab_numbers` (критично, средняя сложность)

**Затем:**
3. Оптимизация условного удаления (если используется)
4. Минимизация копий DataFrame

**Ожидаемый результат:** Сокращение времени выполнения с 44 минут до **3-6 минут** (ускорение **7-15x**)
